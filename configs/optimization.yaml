# =============================================================================
# Optimization Configuration (EnvNet-v2)
# =============================================================================
# Configuration for EnvNet-v2 hyperparameter optimization.
# Model-specific preprocessing settings are automatically applied from model config.
# 
# NOTE: Trainer settings are configured in configs/base_training.yaml
# Override specific settings via command line: trainer.precision=16-mixed trainer.log_every_n_steps=10
# 
# NOTE: Loss function is configured in configs/base_training.yaml
# Override via command line: loss._target_=torch.nn.KLDivLoss loss.reduction=batchmean
# =============================================================================

defaults:
  - base_training
  - dataset: esc50
  - model: envnet_v2
  - _self_

hydra:
  run:
    dir: outputs/optimization/${now:%Y-%m-%d}/${now:%H-%M-%S}
  output_subdir: null

logging:
  experiment_name: envnet_v2_esc50_optimization

# Optuna configuration
optuna:
  # Study configuration
  study_name: "envnet_v2_esc50_optimization"
  direction: "maximize"  # maximize validation accuracy
  storage_path: "optuna_studies.db"  # SQLite database path

  # Optimization settings
  n_trials: 100
  timeout: 14400  # 4 hours in seconds (60*60*4 = 14400) (null for no timeout)

  # TPE Sampler configuration
  sampler:
    n_startup_trials: 10      # Number of random trials before TPE kicks in
    n_ei_candidates: 24       # Number of candidates for Expected Improvement
    seed: 42                  # Random seed for reproducibility

  # Hyperband Pruner configuration
  pruner:
    min_resource: 1           # Start with 1 epoch minimum
    max_resource: 200         # Up to 200 epochs maximum  
    reduction_factor: 2       # Keep top 50%, prune bottom 50%

  # Monitoring configuration
  monitor: "val/acc"          # Metric to optimize
  mode: "max"                 # Maximize the metric
  min_epochs: 20              # Minimum epochs before pruning

  # MLflow integration
  mlflow_experiment_name: "optuna_hyperparameter_optimization_envnet_v2"

  # Output configuration
  output_dir: "outputs/optimization"
  best_config_path: "best_config.yaml" 

trainer:
  max_epochs: 250