# =============================================================================
# Optimization Configuration (EnvNet-v2 Model)
# =============================================================================
# Configuration for EnvNet-v2 training with BC mixing and hyperparameter optimization.
# 
# EXPERIMENT EXAMPLES:
# ====================
# 
# 1. EnvNet-v2 with BC mixing (default):
#    python train.py -cn optimization
#
# 2. EnvNet-v2 without augmentation:
#    python train.py -cn optimization dataset.enable_bc_mixing=false
#
# 3. EnvNet-v2 with custom window length:
#    python train.py -cn optimization dataset.preprocessing_config.window_length=3.0
#
# 4. EnvNet-v2 with different augmentation settings:
#    python train.py -cn optimization dataset.preprocessing_config.augment.gain_shift='[-10,10]'
#
# RECOMMENDED ENVNET-V2 SETTINGS:
# ================================
# - Learning rate: 0.001-0.01 (higher than transformers)
# - Batch size: 32-64 (larger batches work well)
# - Epochs: 200-300 (needs more epochs than AST)
# - Weight decay: 1e-4
# - BC mixing: true (significant improvement)
# - Window length: 3.0-5.0 seconds
# - Loss function: KLDivLoss for BC mixing, CrossEntropyLoss otherwise
# =============================================================================

defaults:
  - base_training
  - dataset: esc50
  - model: envnet_v2
  - _self_

# Override dataset for EnvNet-v2 mode (waveforms)
dataset:
  is_spectrogram: false    # REQUIRED for EnvNet-v2
  enable_bc_mixing: true   # BC mixing for EnvNet-v2 (recommended)
  enable_mixup: false      # CANNOT be true with waveforms (enforced)
  time_mask: false         # Ignored in waveform mode
  freq_mask: false         # Ignored in waveform mode

hydra:
  run:
    dir: outputs/optimization/${now:%Y-%m-%d}/${now:%H-%M-%S}
  output_subdir: null

# Override trainer settings for optimization
trainer:
  max_epochs: 1000
  accelerator: auto        # picks GPU if available
  precision: 16-mixed      # mixed-precision
  devices: 1
  log_every_n_steps: 10    # Log more frequently than default 50

# Optimization-specific loss function
# NOTE: For BC mixing, KLDivLoss often works better than CrossEntropyLoss
# To use KL divergence: loss._target_=torch.nn.KLDivLoss loss.reduction=batchmean
loss:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.0

logging:
  experiment_name: envnet_v2_esc50_optimization_exp

# Optuna configuration
optuna:
  # Study configuration
  study_name: "envnet_v2_esc50_optimization"
  direction: "maximize"  # maximize validation accuracy
  storage_path: "optuna_studies.db"  # SQLite database path

  # Optimization settings
  n_trials: 100
  timeout: null  # No timeout by default (set in seconds if needed)

  # TPE Sampler configuration
  sampler:
    n_startup_trials: 10      # Number of random trials before TPE kicks in
    n_ei_candidates: 24       # Number of candidates for Expected Improvement
    seed: 42                  # Random seed for reproducibility

  # Hyperband Pruner configuration
  pruner:
    min_resource: 1           # Start with 1 epoch minimum
    max_resource: 100         # Up to 100 epochs maximum  
    reduction_factor: 2       # Keep top 50%, prune bottom 50%

  # Monitoring configuration
  monitor: "val/acc"          # Metric to optimize
  mode: "max"                 # Maximize the metric
  min_epochs: 20              # Minimum epochs before pruning

  # MLflow integration
  mlflow_experiment_name: "optuna_hyperparameter_optimization"

  # Output configuration
  output_dir: "outputs/optimization"
  best_config_path: "best_config.yaml" 