# =============================================================================
# Base Training Configuration
# =============================================================================
# Shared settings between training and optimization configurations.
# Contains proven defaults and parameter ranges for audio classification.
#
# MODEL-SPECIFIC RECOMMENDATIONS (based on testing):
# ====================================================
# AST (Audio Spectrogram Transformer):
#   - lr: 0.0005 (lower for transformers)
#   - batch_size: 96-128 (larger for better GPU utilization - transformer bottleneck)
#   - num_workers: 4 (reduced to avoid data loading bottleneck)
#   - max_epochs: 100-200
#   - weight_decay: 1e-6
#   - precision: 16-mixed (transformers handle mixed precision well)
#   - model: deit_small_patch16_224 (faster than deit_base_patch16_384)
#
# EnvNet-v2:
#   - lr: 0.0001 (IMPORTANT: reduced from 0.001 to prevent gradient explosion)
#   - batch_size: 32-64  
#   - max_epochs: 200-300
#   - weight_decay: 1e-4
#   - precision: 32 (IMPORTANT: large model needs 32-bit for stability, 16-bit causes NaN)
#   - gradient_clip_val: 1.0 (recommended to prevent gradient explosion)
#
# USAGE EXAMPLES:
# ===============
# EnvNet-v2 with recommended stable settings:
#   python scripts/train.py model=envnet_v2 trainer.precision=32 optimizer.lr=0.0001
#
# AST with optimized settings (small model + large batch for speed):
#   python scripts/train.py model=ast trainer.precision=16-mixed optimizer.lr=0.0005 batch_size=128 num_workers=4
# =============================================================================

defaults:
  - _self_
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none

seed: 42                  # Range: any integer, for reproducibility

# =============================================================================
# TRAINER CONFIGURATION
# =============================================================================
trainer:
  max_epochs: 250        # Range: 50-1000, AST: 100-200, EnvNet-v2: 200-300
  accelerator: auto       # Options: auto, gpu, cpu, tpu
  precision: 16-mixed     # Options: 32, 16-mixed, bf16-mixed | EnvNet-v2: use 32, AST: 16-mixed
  devices: 1              # Range: 1-8, number of GPUs/devices
  log_every_n_steps: 1   # Range: 10-100, logging frequency | Recommended: 10 for less verbose
  gradient_clip_val: 1.0  # Prevent gradient explosion and reduce memory spikes

# =============================================================================
# OPTIMIZER CONFIGURATION  
# =============================================================================
optimizer:
  _target_: torch.optim.Adam
  lr: 0.0005               # Range: 1e-5 to 1e-2 | AST: 0.0005, EnvNet-v2: 0.0001 (IMPORTANT!)
  weight_decay: 1e-6      # Range: 0 to 1e-3, AST: 1e-6, EnvNet-v2: 1e-4

# =============================================================================
# SCHEDULER CONFIGURATION
# =============================================================================
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${trainer.max_epochs}              # Should match max_epochs for full cycle

# =============================================================================
# LOSS FUNCTION CONFIGURATION
# =============================================================================
# NOTE: Loss function depends on augmentation type:
# - CrossEntropyLoss: for Mixup (handles soft labels automatically)
# - KLDivLoss: for BC mixing (better for soft label distributions)  
# - Standard losses: for no augmentation
loss:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.0    # Range: 0.0-0.2, adds regularization

# =============================================================================
# METRICS CONFIGURATION
# =============================================================================
metric:
  _target_: torchmetrics.classification.Accuracy
  task: multiclass
  num_classes: ${dataset.num_classes}

# =============================================================================
# TRAINING PARAMETERS
# =============================================================================
# NOTE: These are the SINGLE SOURCE OF TRUTH for batch_size and num_workers
# They are passed directly to the datamodule constructor by the training script
# DO NOT duplicate these values in dataset config files
batch_size: 128            # Range: 8-128, AST: 96-128 (large), EnvNet-v2: 32-64
num_workers: 8            # Range: 0-16, 8 is optimal

# =============================================================================
# CHECKPOINTING CONFIGURATION
# =============================================================================
checkpoint:
  monitor: val/acc        # Options: val/acc, val/loss, train/loss
  mode: max               # max for accuracy, min for loss
  dirpath: checkpoints
  save_top_k: 1          # Range: 1-10, number of best checkpoints to keep
  filename: "epoch-{epoch:02d}-val_acc-{val/acc:.3f}"

# =============================================================================
# EARLY STOPPING CONFIGURATION
# =============================================================================
early_stop:
  monitor: val/acc        # Same as checkpoint monitor
  mode: max               # Same as checkpoint mode  
  patience: 30            # Range: 5-50, AST: 15, EnvNet-v2: 40
  min_delta: 0.001        # Range: 0.0001-0.01, minimum improvement threshold

# =============================================================================
# TROUBLESHOOTING NOTES
# =============================================================================
# Common Issues & Solutions:
# 
# 1. NaN Losses (EnvNet-v2):
#    - Cause: Mixed precision (16-bit) causes numerical instability  
#    - Solution: Use trainer.precision=32
#
# 2. High Loss/Poor Learning (EnvNet-v2):
#    - Cause: Learning rate too high (0.001 causes gradient explosion)
#    - Solution: Use optimizer.lr=0.0001
#
# 3. Gradient Explosion:
#    - Solution: Add +trainer.gradient_clip_val=1.0
#
# 4. Turn off all augmentation:
#    - EnvNet-v2: dataset.enable_bc_mixing=false model.dataset_overrides.preprocessing_config.augment.time_stretch=null model.dataset_overrides.preprocessing_config.augment.gain_shift=null
#    - AST: dataset.enable_mixup=false model.dataset_overrides.augment.time_mask=false model.dataset_overrides.augment.freq_mask=false 